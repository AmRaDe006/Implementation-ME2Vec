{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9860\\797207695.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPickleUtils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# from src.utils import PickleUtils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGAT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Semester 7 (4th yr)\\BTP\\sourceCode\\ME2Vec\\src\\gat.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# from .layers import GraphAttentionLayer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGraphAttentionLayer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mGAT_3L\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'layers'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from src.utils import PickleUtils\n",
    "# from src.gat import GAT\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../\")\n",
    "# s_dir = os.path.dirname(__file__)\n",
    "# module_dir = os.path.join(s_dir, \"..\", \"src\")\n",
    "# sys.path.append(module_dir)\n",
    "\n",
    "from src.utils import PickleUtils\n",
    "# from src.utils import PickleUtils\n",
    "from src.gat import GAT\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import BatchSampler, RandomSampler, SequentialSampler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "dev = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--seed', type=int, default=1986,\n",
    "                        help='global random seed number')\n",
    "\n",
    "    parser.add_argument('--epochs', type=int, default=100,\n",
    "                        help='number of epochs of training')\n",
    "\n",
    "    parser.add_argument('--lr', type=float, default=0.01,\n",
    "                        help='learning rate')\n",
    "\n",
    "    parser.add_argument('--lr-factor', type=float, default=0.2,\n",
    "                        help='rate of reducing learning rate')\n",
    "\n",
    "    parser.add_argument('--lr-patience', type=int, default=3,\n",
    "                        help='number of epochs validation loss not improving')\n",
    "\n",
    "    parser.add_argument('--batch-size', type=int, default=512)\n",
    "\n",
    "    parser.add_argument('--log-interval', type=int, default=20)\n",
    "\n",
    "    parser.add_argument('--weight-decay', type=float, default=0.)\n",
    "\n",
    "    parser.add_argument('--nb-heads', type=int, default=4,\n",
    "                        help='number of attention heads')\n",
    "\n",
    "    parser.add_argument('--dropout', type=float, default=0.6)\n",
    "\n",
    "    parser.add_argument('--alpha', type=float, default=0.2,\n",
    "                        help='parameters of GAT')\n",
    "\n",
    "    parser.add_argument('--checkpoint', dest='checkpoint', action='store_true')\n",
    "    parser.set_defaults(weighted=True)\n",
    "\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer, args, doc_emb, doc_spec, doc_svc, svc_emb):\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    idx_list = list(BatchSampler(RandomSampler(\n",
    "        range(len(doc_emb))), args.batch_size, drop_last=False))\n",
    "    doc_emb_ts = torch.tensor(doc_emb, dtype=torch.float, device=dev)\n",
    "    doc_spec_ts = torch.tensor(doc_spec, dtype=torch.long, device=dev)\n",
    "    doc_svc_ts = torch.tensor(doc_svc, dtype=torch.long, device=dev)\n",
    "\n",
    "    for i in range(len(idx_list)):\n",
    "        x = doc_emb_ts[idx_list[i]]\n",
    "        y = doc_spec_ts[idx_list[i]]\n",
    "        adj = doc_svc_ts[idx_list[i]]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred_y, _ = model(x, adj, svc_emb)\n",
    "        loss = F.cross_entropy(pred_y, y)\n",
    "        train_loss += loss.item() * len(x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, (i+1) * args.batch_size, len(doc_emb),\n",
    "                100. * (i+1) * args.batch_size / len(doc_emb), loss.item()))\n",
    "\n",
    "    train_loss /= len(doc_emb)\n",
    "    print('Average train loss of epoch {} is {:.4f}.\\n'.format(epoch, train_loss))\n",
    "\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, model, optimizer, args, doc_emb, doc_spec, doc_svc, svc_emb):\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    idx_list = list(BatchSampler(SequentialSampler(\n",
    "        range(len(doc_emb))), args.batch_size, drop_last=False))\n",
    "    doc_emb_ts = torch.tensor(doc_emb, dtype=torch.float, device=dev)\n",
    "    doc_spec_ts = torch.tensor(doc_spec, dtype=torch.long, device=dev)\n",
    "    doc_svc_ts = torch.tensor(doc_svc, dtype=torch.long, device=dev)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(idx_list)):\n",
    "            x = doc_emb_ts[idx_list[i]]\n",
    "            y = doc_spec_ts[idx_list[i]]\n",
    "            adj = doc_svc_ts[idx_list[i]]\n",
    "\n",
    "            pred_y, _ = model(x, adj, svc_emb)\n",
    "            test_loss += F.cross_entropy(pred_y, y, reduction='sum')\n",
    "            pred = F.log_softmax(pred_y, dim=1).argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "\n",
    "            if i % args.log_interval == 0:\n",
    "                print('Test Epoch: {} [{}/{} ({:.0f}%)]'.format(\n",
    "                    epoch, (i+1) * args.batch_size, len(doc_emb),\n",
    "                    100. * (i+1) * args.batch_size / len(doc_emb)))\n",
    "\n",
    "    test_loss /= len(doc_emb)\n",
    "    accu = 100. * correct / len(doc_emb)\n",
    "\n",
    "    print('Average test loss of epoch {} is {:.4f}, accuracy is {:.2f}%.\\n'.format(\n",
    "        epoch, test_loss, accu))\n",
    "\n",
    "    return test_loss, accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_batch(model, args, doc_emb, doc_svc, svc_emb):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    idx_list = list(BatchSampler(SequentialSampler(\n",
    "        range(len(doc_emb))), args.batch_size, drop_last=False))\n",
    "\n",
    "    x = doc_emb[idx_list[0]]\n",
    "    adj = doc_svc[idx_list[0]]\n",
    "    _, x_prime = model(x, adj, svc_emb)\n",
    "    doc_emb_prime = x_prime.detach().cpu().numpy()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(idx_list)):\n",
    "            x = doc_emb[idx_list[i]]\n",
    "            adj = doc_svc[idx_list[i]]\n",
    "            _, x_prime = model(x, adj, svc_emb)\n",
    "            doc_emb_prime = x_prime.detach().cpu().numpy()\n",
    "\n",
    "            if i == 0:\n",
    "                doc_emb_prime = x_prime.detach().cpu().numpy()\n",
    "            else:\n",
    "                doc_emb_prime = np.concatenate(\n",
    "                    (doc_emb_prime, x_prime.detach().cpu().numpy()), axis=0)\n",
    "\n",
    "            if i % args.log_interval == 0:\n",
    "                print('Processed: [{}/{} ({:.0f}%)]'.format(\n",
    "                    (i+1) * args.batch_size, len(doc_emb),\n",
    "                    100. * (i+1) * args.batch_size / len(doc_emb)))\n",
    "\n",
    "    return doc_emb_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_rnd_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed(args.seed)# type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_emb(args):\n",
    "    # load prepared init data for training doctor embedding\n",
    "    doc_emb_data = pd.read_parquet('saved_data/spec_init_emb.parquet')\n",
    "\n",
    "    doc_emb = np.stack(doc_emb_data.embedding.to_list(), axis=0)\n",
    "    doc_spec = doc_emb_data.spec_id.values\n",
    "    doc_svc = doc_emb_data.svc_id.values\n",
    "\n",
    "    # load service embedding matrix\n",
    "    ppd_emb = np.loadtxt('node2vec/emb/ppd_eICU.emd', skiprows=1)\n",
    "    ppd_coor = np.array([x[1:] for x in ppd_emb])\n",
    "    ppd_id = [int(x[0]) for x in ppd_emb]\n",
    "\n",
    "    # sort ppd_emb in the ascending order of node id\n",
    "    svc_emb = ppd_coor[np.argsort(ppd_id), :]\n",
    "    svc_emb_ts = torch.tensor(svc_emb, dtype=torch.float, device=dev)\n",
    "\n",
    "    # prepare dataset\n",
    "    num_doc = len(doc_emb_data)  # len(doc_emb)\n",
    "    num_svc = len(svc_emb)\n",
    "    adj_mat = np.zeros((num_doc, num_svc), dtype=int)\n",
    "    for i in range(num_doc):\n",
    "        adj_mat[i, doc_svc[i]] = 1\n",
    "\n",
    "    doc_emb_ts = torch.tensor(doc_emb, dtype=torch.float, device=dev)\n",
    "    doc_svc_ts = torch.tensor(adj_mat, dtype=torch.long, device=dev)\n",
    "\n",
    "    # define and load models\n",
    "    model = GAT(\n",
    "        nfeat=doc_emb.shape[1],\n",
    "        nhid=int(doc_emb.shape[1] / args.nb_heads),\n",
    "        nclass=len(np.unique(doc_spec)),\n",
    "        dropout=args.dropout,\n",
    "        drop_enc=args.drop_enc,\n",
    "        alpha=args.alpha,\n",
    "        nheads=args.nb_heads).to(dev)\n",
    "\n",
    "    # load model parameters\n",
    "    checkpoint = torch.load('saved_models/doc_emb_best')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    # inference\n",
    "    doc_emb_prime = test_batch(model, args, doc_emb_ts, doc_svc_ts, svc_emb_ts)\n",
    "    PickleUtils.saver('saved_data/doc_emb_prime.pkl', doc_emb_prime)\n",
    "\n",
    "    doc_emb = np.stack(doc_emb_data.embedding.to_list(), axis=0)\n",
    "    doc_spec = doc_emb_data.spec_id.values\n",
    "    doc_svc = doc_emb_data.svc_id.values\n",
    "\n",
    "    spec_emb = np.zeros((49, 128), dtype=float)\n",
    "    for i in range(49):\n",
    "        spec_emb[i] = np.mean(doc_emb_prime[np.where(doc_spec == i)], axis=0)\n",
    "    PickleUtils.saver('saved_data/spec_emb.pkl', spec_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load prepared init data for training doctor embedding\n",
    "doc_emb_data = pd.read_parquet('saved_data/spec_init_emb.parquet')\n",
    "\n",
    "doc_emb = np.stack(doc_emb_data.embedding.to_list(), axis=0)\n",
    "doc_spec = doc_emb_data.spec_id.values\n",
    "doc_svc = doc_emb_data.svc_id.values\n",
    "\n",
    "# load service embedding matrix\n",
    "ppd_emb = np.loadtxt('node2vec/emb/ppd_eICU.emd', skiprows=1)\n",
    "ppd_coor = np.array([x[1:] for x in ppd_emb])\n",
    "ppd_id = [int(x[0]) for x in ppd_emb]\n",
    "\n",
    "# sort ppd_emb in the ascending order of node id\n",
    "svc_emb = ppd_coor[np.argsort(ppd_id), :]\n",
    "svc_emb_ts = torch.tensor(svc_emb, dtype=torch.float, device=dev)\n",
    "\n",
    "# prepare dataset\n",
    "num_doc = len(doc_emb_data)\n",
    "num_svc = len(svc_emb)\n",
    "set_rnd_seed(args)\n",
    "rndx = np.random.permutation(range(num_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_emb = doc_emb[rndx]\n",
    "doc_spec = doc_spec[rndx]\n",
    "doc_svc = doc_svc[rndx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_mat = np.zeros((num_doc, num_svc), dtype=int)\n",
    "for i in range(num_doc):\n",
    "    adj_mat[i, doc_svc[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "doc_emb_train = doc_emb[:round(num_doc * args.train_ratio)]\n",
    "doc_emb_test = doc_emb[round(num_doc * args.train_ratio):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_spec_train = doc_spec[:round(num_doc * args.train_ratio)]\n",
    "doc_spec_test = doc_spec[round(num_doc * args.train_ratio):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_svc_train = adj_mat[:round(num_doc * args.train_ratio)]\n",
    "doc_svc_test = adj_mat[round(num_doc * args.train_ratio):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "set_rnd_seed(args)\n",
    "model = GAT(\n",
    "    nfeat=doc_emb.shape[1],\n",
    "    nhid=int(doc_emb.shape[1] / args.nb_heads),\n",
    "    nclass=len(np.unique(doc_spec)),\n",
    "    dropout=args.dropout,\n",
    "    drop_enc=args.drop_enc,\n",
    "    alpha=args.alpha,\n",
    "    nheads=args.nb_heads).to(dev)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr,\n",
    "                        weight_decay=args.weight_decay)\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer, factor=args.lr_factor, patience=args.lr_patience, verbose=True)\n",
    "\n",
    "# train and validation\n",
    "best_loss = 100.\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train_loss = train(epoch, model, optimizer, args,\n",
    "                        doc_emb_train, doc_spec_train, doc_svc_train, svc_emb_ts)\n",
    "    test_loss, accu = test(epoch, model, optimizer, args,\n",
    "                            doc_emb_test, doc_spec_test, doc_svc_test, svc_emb_ts)\n",
    "    scheduler.step(test_loss)\n",
    "\n",
    "    if test_loss < best_loss:\n",
    "        best_loss = test_loss\n",
    "\n",
    "        if args.checkpoint:\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'test_loss': test_loss,\n",
    "                'accu': accu,\n",
    "                'epoch': epoch\n",
    "            }, 'saved_models/doc_emb_best')\n",
    "\n",
    "get_doc_emb(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_btp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82bc327abbbadcca180f52d1cedf78658642944fd6e53b8c21c3b269f6adbb8a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
